{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Notebook for Base Models\n",
    "\n",
    "### Contains Pipelines for Random Forrest, SVC and XGBoost Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier,StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import own modules\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from scripts import features as ft\n",
    "from scripts import preprocessing as pp\n",
    "from scripts import evaluate_models as em\n",
    "\n",
    "# plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')\n",
    "plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Dataframe with Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> dataframe has 7598 instances and 45 columns\n",
      " -> there are 45 numerical columns\n",
      " -> there are 0 categoricals columns\n"
     ]
    }
   ],
   "source": [
    "# path to csv file\n",
    "path_df = os.path.join(\"..\", \"data\", \"df_deep_sam.csv\")\n",
    "\n",
    "# get features - or recalculate\n",
    "recalculate_df = False\n",
    "if os.path.isfile(path_df) and not recalculate_df:\n",
    "    df = pd.read_csv(path_df)\n",
    "else:\n",
    "    df = ft.get_features()\n",
    "    df.to_csv(path_df, index=False)\n",
    "\n",
    "# set id as index\n",
    "df = df.set_index(\"id\", drop=True)\n",
    "\n",
    "# drop first batch of useless variables\n",
    "df = df.drop(columns=['img', 'sp_idx'])\n",
    "df = df.drop(columns=[col for col in df.columns if \"_obj\" in col])  # drop 'object' columns\n",
    "\n",
    "# find numerical and categorical columns\n",
    "num_cols = df.columns[df.dtypes != \"object\"]\n",
    "cat_cols = df.columns[df.dtypes == \"object\"]\n",
    "\n",
    "# print info\n",
    "print(f\" -> dataframe has {df.shape[0]} instances and {df.shape[1]} columns\")\n",
    "print(f\" -> there are {len(num_cols)} numerical columns\")\n",
    "print(f\" -> there are {len(cat_cols)} categoricals columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Columns we need and produce Feature Lists for each Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Lists for the Features each Model uses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 Features on Best SVC Model\n",
    "svc_feature_list = [\"sp_fix_duration_ms_total\",\"sp_fix_duration_ms_mean\",\"sp_fix_duration_ms_var\", \"sam_sal_first_fixation\",\"sam_sal_sum\",\n",
    "                    \"sam_sal_KLD\", \"obj_t_abs_on_background\",\"obj_t_abs_on_animate\", \"obj_n_fix_background\",\"obj_n_fix_inanimate\",\n",
    "                    \"obj_n_fix_animate\"]\n",
    "\n",
    "# Features on Best XGB Model\n",
    "xgb_feature_list = ['sp_fix_count', 'sp_fix_duration_ms_var', 'sp_len_px_total',\n",
    "       'sp_saccade_amplitude_px_mean', 'sp_saccade_amplitude_px_var',\n",
    "       'sp_distance_to_centre_px_mean', 'sp_distance_to_centre_px_var',\n",
    "       'sp_distance_to_sp_mean_px_mean', 'sp_distance_to_sp_mean_px_var',\n",
    "       'dg_sal_first_fixation', 'dg_sal_sum', 'dg_sal_max', 'dg_sal_weighted_duration_sum',\n",
    "       'dg_sal_weighted_duration_mean', 'dg_sal_KLD', 'dg_sal_NSS', 'obj_t_abs_on_face',\n",
    "       'obj_t_rel_on_face', 'obj_t_abs_on_animate', 'obj_t_abs_on_inanimate',\n",
    "       'obj_t_abs_on_background', 'obj_t_rel_on_animate',\n",
    "       'obj_t_rel_on_inanimate', 'obj_t_rel_on_background']\n",
    "\n",
    "# Best Features for Random Forrest Model\n",
    "rf_feature_list = ['sp_fix_count', 'sp_fix_duration_ms_total', 'sp_fix_duration_ms_mean',\n",
    " 'sp_fix_duration_ms_var', 'sp_len_px_total', 'sp_saccade_amplitude_px_mean',\n",
    " 'sp_saccade_amplitude_px_var', 'sp_distance_to_centre_px_mean',\n",
    " 'sp_distance_to_centre_px_var', 'sp_distance_to_sp_mean_px_mean',\n",
    " 'sp_distance_to_sp_mean_px_var', 'dg_sal_first_fixation', 'dg_sal_mean',\n",
    " 'dg_sal_sum', 'dg_sal_max', 'dg_sal_weighted_duration_sum',\n",
    " 'dg_sal_weighted_duration_mean', 'dg_sal_KLD', 'dg_sal_NSS',\n",
    " 'obj_t_abs_on_animate', 'obj_t_abs_on_background']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Dropping Columns and Give back X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the X for each Model based on it's features\n",
    "\n",
    "def feature_selector(df, features_to_keep):\n",
    "    # Select features\n",
    "    X = df[features_to_keep]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers, Preproccessing and Pipeline for our 3 Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Transformer for SVC with my Custom Function and the Scaler\n",
    "\n",
    "transformer_svc = [(\"feature_selector\", FunctionTransformer(feature_selector, validate=False),svc_feature_list),\n",
    "                   (\"scaler\", StandardScaler(),'X')\n",
    "                   ]\n",
    "\n",
    "# Wrap a ColumnTransformer around the transformer_svc\n",
    "\n",
    "preprocessing_svc = ColumnTransformer(transformer_svc, remainder=\"drop\")\n",
    "\n",
    "# pack the preprocessing steps into a Pipeline\n",
    "svc_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing_svc),\n",
    "    (\"classifier\", SVC(C=0.1, degree=4, kernel='poly',gamma='scale', probability=True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Transformer for SVC with my Custom Function and the Scaler\n",
    "\n",
    "transformer_xgb = [(\"feature_selector\", FunctionTransformer(feature_selector, validate=False),xgb_feature_list),\n",
    "                   (\"scaler\", MinMaxScaler(),'X')\n",
    "                   ]\n",
    "\n",
    "# Wrap a ColumnTransformer around the transformer_svc\n",
    "\n",
    "preprocessing_xgb = ColumnTransformer(transformer_xgb, remainder=\"drop\")\n",
    "\n",
    "# pack the preprocessing steps into a Pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing_xgb),\n",
    "    (\"classifier\", XGBClassifier(learning_rate = 0.01, max_depth=5, n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Transformer for SVC with my Custom Function and the Scaler\n",
    "\n",
    "transformer_rf = [(\"feature_selector\", FunctionTransformer(feature_selector, validate=False),rf_feature_list),\n",
    "                   \n",
    "                   ]\n",
    "\n",
    "# Wrap a ColumnTransformer around the transformer_svc\n",
    "\n",
    "preprocessing_rf = ColumnTransformer(transformer_rf, remainder=\"drop\")\n",
    "\n",
    "# pack the preprocessing steps into a Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing_rf),\n",
    "    (\"classifier\", RandomForestClassifier(max_depth=7, max_features=\"sqrt\", min_samples_leaf=40, min_samples_split=50,n_estimators=50, verbose=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare X and y for Different Models\n",
    "- SVC\n",
    "- XGBoost\n",
    "- RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For SVC Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and target\n",
    "X = df[svc_feature_list]\n",
    "y = df.pop(\"asd\")\n",
    "\n",
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = pp.split(X, y)\n",
    "\n",
    "# print info\n",
    "print(f\"train-set has '{len(y_train)}' samples & '{X.shape[1]}' features\")\n",
    "print(f\"test-set has '{len(y_test)}' samples - out of '{df.shape[0]}'\")\n",
    "print(f\"  ~ {len(y_test) / df.shape[0] * 100:.2f}% of full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and target\n",
    "X = df[xgb_feature_list]\n",
    "y = df.pop(\"asd\")\n",
    "\n",
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = pp.split(X, y)\n",
    "\n",
    "# print info\n",
    "print(f\"train-set has '{len(y_train)}' samples & '{X.shape[1]}' features\")\n",
    "print(f\"test-set has '{len(y_test)}' samples - out of '{df.shape[0]}'\")\n",
    "print(f\"  ~ {len(y_test) / df.shape[0] * 100:.2f}% of full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Pipeline\n",
    "### 2 Versions of the Stacking Pipeline\n",
    "- in 1 Version you can run it and it will compute all steps (long runtime)\n",
    "- in 2 Version you load the pickle files of the base Model and perform Grid Search on the Logistic Regression Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Version full run! Only do this if u have lots of time !!!\n",
    "\n",
    "- We have 2 Pipelines here 1 gives us out the Proba of our master model for each Class\n",
    "\n",
    "- The other Pipeline is for Predicting 0 or 1  \n",
    "\n",
    "i added the Proba pipeline due to better error analysis reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'final_estimator__C': np.logspace(-3, 3, 10),  # Reduced values for C\n",
    "    'final_estimator__penalty': ['l1', 'l2'],  # Removed 'elasticnet'\n",
    "    'final_estimator__solver': ['liblinear', 'lbfgs', 'saga'],  # Removed 'newton-cg', 'sag'\n",
    "    'final_estimator__class_weight': [None, 'balanced'],  # Class weights\n",
    "    'final_estimator__max_iter': [50, 100, 200],  # Reduced maximum number of iterations\n",
    "    'final_estimator__tol': [1e-4, 1e-3],  # Reduced tolerance for stopping criteria\n",
    "    'final_estimator__fit_intercept': [True, False],  # Whether to calculate the intercept\n",
    "    'final_estimator__l1_ratio': [None, 0.1, 0.5, 0.9]  # Elastic-Net mixing parameter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StackingClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m master_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Stacking Classifier Pipeline\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m stacking_pipeline_proba \u001b[38;5;241m=\u001b[39m \u001b[43mStackingClassifier\u001b[49m(\n\u001b[1;32m      6\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m'\u001b[39m, svc_pipeline),\n\u001b[1;32m      8\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb_pipeline),\n\u001b[1;32m      9\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, rf_pipeline)\n\u001b[1;32m     10\u001b[0m     ],\n\u001b[1;32m     11\u001b[0m     final_estimator\u001b[38;5;241m=\u001b[39mmaster_model,\n\u001b[1;32m     12\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     13\u001b[0m     stack_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     passthrough\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     \n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     19\u001b[0m stacking_pipeline_predict \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m     20\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     21\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m'\u001b[39m, svc_pipeline),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \n\u001b[1;32m     30\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StackingClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Define an empty Master Model - LogReg\n",
    "master_model = LogisticRegression()\n",
    "\n",
    "# Stacking Classifier Pipeline\n",
    "stacking_pipeline_proba = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svc', svc_pipeline),\n",
    "        ('xgb', xgb_pipeline),\n",
    "        ('rf', rf_pipeline)\n",
    "    ],\n",
    "    final_estimator=master_model,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba',\n",
    "    passthrough=True,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "stacking_pipeline_predict = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svc', svc_pipeline),\n",
    "        ('xgb', xgb_pipeline),\n",
    "        ('rf', rf_pipeline)\n",
    "    ],\n",
    "    final_estimator=master_model,\n",
    "    cv=5,\n",
    "    stack_method='predict',\n",
    "    passthrough=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter tuning of the master model\n",
    "grid_search_pipeline = GridSearchCV(\n",
    "    estimator=stacking_pipeline_predict,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    verbose=2\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2 - Easy Version with loaded Pickle Files!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base models from pickle files\n",
    "with open('path_to_svc_pipeline.pkl', 'rb') as file:\n",
    "    svc_pipeline = pickle.load(file)\n",
    "\n",
    "with open('path_to_xgb_pipeline.pkl', 'rb') as file:\n",
    "    xgb_pipeline = pickle.load(file)\n",
    "\n",
    "with open('path_to_rf_pipeline.pkl', 'rb') as file:\n",
    "    rf_pipeline = pickle.load(file)\n",
    "\n",
    "# Define an empty Master Model - LogReg\n",
    "master_model = LogisticRegression()\n",
    "\n",
    "# Stacking Classifier Pipeline with predict_proba as stack_method\n",
    "stacking_pipeline_proba = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svc', svc_pipeline),\n",
    "        ('xgb', xgb_pipeline),\n",
    "        ('rf', rf_pipeline)\n",
    "    ],\n",
    "    final_estimator=master_model,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba',  # Use predict_proba for classification tasks\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# Stacking Classifier Pipeline with predict as stack_method\n",
    "stacking_pipeline_predict = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svc', svc_pipeline),\n",
    "        ('xgb', xgb_pipeline),\n",
    "        ('rf', rf_pipeline)\n",
    "    ],\n",
    "    final_estimator=master_model,\n",
    "    cv=5,\n",
    "    stack_method='predict',  # Use predict for classification tasks\n",
    "    passthrough=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'final_estimator__C': np.logspace(-3, 3, 10),  # Reduced values for C\n",
    "    'final_estimator__penalty': ['l1', 'l2'],  # Removed 'elasticnet'\n",
    "    'final_estimator__solver': ['liblinear', 'lbfgs', 'saga'],  # Removed 'newton-cg', 'sag'\n",
    "    'final_estimator__class_weight': [None, 'balanced'],  # Class weights\n",
    "    'final_estimator__max_iter': [50, 100, 200],  # Reduced maximum number of iterations\n",
    "    'final_estimator__tol': [1e-4, 1e-3],  # Reduced tolerance for stopping criteria\n",
    "    'final_estimator__fit_intercept': [True, False],  # Whether to calculate the intercept\n",
    "    'final_estimator__l1_ratio': [None, 0.1, 0.5, 0.9]  # Elastic-Net mixing parameter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter tuning of the master model\n",
    "grid_search_pipeline = GridSearchCV(\n",
    "    estimator=stacking_pipeline_predict,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    verbose=2\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
