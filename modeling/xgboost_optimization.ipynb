{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import own modules\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from scripts import features as ft\n",
    "from scripts import preprocessing as pp\n",
    "from scripts import evaluate_models as em\n",
    "\n",
    "#plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')\n",
    "plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparations\n",
    "\n",
    "### Load the CSV into a Dataframe\n",
    "\n",
    "- load csv (or calculate again if not in data folder)\n",
    "- update index=id\n",
    "- drop useless columns\n",
    "- find numerical & object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv file\n",
    "path_df = os.path.join(\"..\", \"data\", \"df_deepgaze2e.csv\")\n",
    "\n",
    "# get features - or recalculate\n",
    "recalculate_df = False\n",
    "if os.path.isfile(path_df) and not recalculate_df:\n",
    "    df = pd.read_csv(path_df)\n",
    "else:\n",
    "    df = ft.get_features()\n",
    "    df.to_csv(path_df, index=False)\n",
    "\n",
    "# set id as index\n",
    "df = df.set_index(\"id\", drop=True)\n",
    "\n",
    "# drop first batch of useless variables\n",
    "df = df.drop(columns=['img', 'sp_idx'])\n",
    "df = df.drop(columns=[col for col in df.columns if \"_obj\" in col])  # drop 'object' columns\n",
    "\n",
    "# find numerical and categorical columns\n",
    "num_cols = df.columns[df.dtypes != \"object\"]\n",
    "cat_cols = df.columns[df.dtypes == \"object\"]\n",
    "\n",
    "# print info\n",
    "print(f\" -> dataframe has {df.shape[0]} instances and {df.shape[1]} columns\")\n",
    "print(f\" -> there are {len(num_cols)} numerical columns\")\n",
    "print(f\" -> there are {len(cat_cols)} categoricals columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for highly correlated columns\n",
    "think after running this lines, which column to additionally drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the correlation matrix\n",
    "corr_matrix = df[num_cols].corr()\n",
    "\n",
    "# Find pairs with correlation >= 0.8\n",
    "high_corr_pairs = np.column_stack(\n",
    "    np.where((np.abs(corr_matrix) >= 0.8) & (corr_matrix != 1))\n",
    ")\n",
    "high_corr_cols = []\n",
    "\n",
    "# Extracting and printing the pairs\n",
    "seen_pairs = set()\n",
    "for i, j in high_corr_pairs:\n",
    "    col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
    "    if (col2, col1) not in seen_pairs:\n",
    "        print(\n",
    "            f\"Correlation between {col1} and {col2} is {round(corr_matrix.iloc[i, j], 3)}\"\n",
    "        )\n",
    "        seen_pairs.add((col1, col2))\n",
    "        seen_pairs.add((col2, col1))\n",
    "        high_corr_cols.append(col1)\n",
    "        high_corr_cols.append(col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further processing\n",
    "\n",
    "... drop more columns, create new ones, handle highly correlating columns....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train & test sets\n",
    "\n",
    "As soon as the dataset is in its final form, perform train-test-split with our own split function to have out 30-image-set always as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and target\n",
    "X = df\n",
    "y = X.pop(\"asd\")\n",
    "num_cols = X.columns[X.dtypes != \"object\"]\n",
    "cat_cols = X.columns[X.dtypes == \"object\"]\n",
    "\n",
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = pp.split(X, y)\n",
    "\n",
    "# print info\n",
    "print(f\"train-set has '{len(y_train)}' samples & '{X.shape[1]}' features\")\n",
    "print(f\"test-set has '{len(y_test)}' samples - out of '{df.shape[0]}'\")\n",
    "print(f\"  ~ {len(y_test) / df.shape[0] * 100:.2f}% of full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables\n",
    "\n",
    "- define `metric`\n",
    "- behavior for saving models as pickles\n",
    "- defaults for model-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# defaults\n",
    "RSEED = 42\n",
    "cv = 10\n",
    "n_jobs = -1\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model - Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformers for Data Preproccessing\n",
    "these are only used for certain Models, which we expect to perform better with Preproccessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add other transformations at the end if needed\n",
    "transformer = [(\"scaler\", MinMaxScaler(), num_cols),\n",
    "               (\"ohe\", OneHotEncoder(drop=\"first\"), cat_cols  )]\n",
    "               \n",
    "preprocessing = ColumnTransformer(transformer,\n",
    "                                  remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pipelines for each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: no scaling / no encoding\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"classifier\", RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# XGBoost: apply scaling / encoding\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing),\n",
    "    (\"classifier\", XGBClassifier())\n",
    "])\n",
    "\n",
    "# Support Vector Classifier: apply scaling / encoding\n",
    "svc_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing),\n",
    "    (\"classifier\",SVC())\n",
    "])\n",
    "\n",
    "# Logistic Regression: apply scaling / encoding\n",
    "log_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessing),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "param_grid_rf = {\n",
    "    \"classifier__n_estimators\": [100, 200, 300],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 5, 10],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "param_grid_xgb = {\n",
    "    \"classifier__n_estimators\": [100, 200, 300],\n",
    "    \"classifier__max_depth\": [3, 5, 7, None],\n",
    "    \"classifier__learning_rate\": [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Support Vector Classifier\n",
    "param_grid_svc = {\n",
    "    'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel types to try\n",
    "    'classifier__C': [0.1, 1, 10, 100],  # Regularization parameter values\n",
    "    'classifier__gamma': ['scale', 'auto'],  # Gamma parameter for RBF kernel\n",
    "    'classifier__degree': [2, 3, 4]  # Degree of the polynomial kernel (only for poly kernel)\n",
    "}\n",
    "\n",
    "# Logistic Regression\n",
    "param_grid_logreg = {\n",
    "    'classifier__penalty': ['l1', 'l2'],  # Penalty type: l1 (Lasso) or l2 (Ridge)\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]  # Regularization strength\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modeling\n",
    "\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV object\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=cv,\n",
    "    scoring=ftwo_scorer,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    "    error_score='raise'\n",
    ")\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_est_xgb = grid_search_xgb.best_estimator_\n",
    "print(\"Best params for XGB are:\", best_params_xgb)\n",
    "print(\"Best est for XGB are:\", best_est_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict & proba\n",
    "pred_test = grid_search_xgb.predict(X_test)\n",
    "proba_test = grid_search_xgb.predict_proba(X_test)\n",
    "\n",
    "pred_train = grid_search_xgb.predict(X_train)\n",
    "proba_train = grid_search_xgb.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "em.report(\n",
    "    y_train=y_train,\n",
    "    y_train_pred=pred_train,\n",
    "    y_train_proba=proba_train,\n",
    "    y_test=y_test,\n",
    "    y_test_pred=pred_test,\n",
    "    y_test_proba=proba_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves - for one model - f2 score\n",
    "em.learning(best_est_xgb, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.model_info(best_est_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature importance using permutation importance\n",
    "* performs better, when model in overfitting\n",
    "* Train a  model and record the score on the validation set. Re-shuffle values for one feature, use the model to predict again, and calculate scores on the validation set. The feature importance for the feature is the difference between the baseline in 1 and the permutation score in 2. Repeat the process for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "#calculate permutation importance for test data \n",
    "result_test = permutation_importance(\n",
    "    grid_search_xgb, X_test, y_test, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_test = result_test.importances_mean.argsort()\n",
    "importances_test = pd.DataFrame(\n",
    "    result_test.importances[sorted_importances_idx_test].T,\n",
    "    columns=X.columns[sorted_importances_idx_test],\n",
    ")\n",
    "\n",
    "#calculate permutation importance for training data \n",
    "result_train = permutation_importance(\n",
    "    grid_search_xgb, X_train, y_train, n_repeats=20, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx_train = result_train.importances_mean.argsort()\n",
    "importances_train = pd.DataFrame(\n",
    "    result_train.importances[sorted_importances_idx_train].T,\n",
    "    columns=X.columns[sorted_importances_idx_train],\n",
    ")\n",
    "\n",
    "f, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "importances_test.plot.box(vert=False, whis=10, ax = axs[0])\n",
    "axs[0].set_title(\"Permutation Importances (test set)\")\n",
    "axs[0].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[0].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[0].figure.tight_layout()\n",
    "\n",
    "importances_train.plot.box(vert=False, whis=10, ax = axs[1])\n",
    "axs[1].set_title(\"Permutation Importances (train set)\")\n",
    "axs[1].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "axs[1].set_xlabel(\"Decrease in accuracy score\")\n",
    "axs[1].figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X axis shows decrease in accuracy score, when feature is shuffled. --> features with higher negative values are more important for performance of model. Features with positive values: not so important.\n",
    "\n",
    "Difference between training and test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features:\n",
    "sal_weighted_duration_sum, sp_fix_duration_ms_total, sp_fix_count, sal_max, sal_first_above_0.9*max_rank, sal_sum, sp_distance_to_centre_px_mean, obj_t_rel_on_inanimate, obj_n_fix_animate, object_n_fix_face, obj-n_fix_inanimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run XGBoost with selected features \n",
    "features_to_remove = ['sal_weighted_duration_sum', 'sp_fix_duration_ms_total', 'sp_fix_count', \n",
    "                      'sal_max', 'sal_first_above_0.9*max_rank', 'sal_sum', \n",
    "                      'sp_distance_to_centre_px_mean', 'obj_t_rel_on_inanimate', \n",
    "                      'obj_n_fix_animate', 'obj_n_fix_face', 'obj_n_fix_inanimate']\n",
    "\n",
    "# Drop the specified features\n",
    "df2 = df.drop(features_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and target\n",
    "X2 = df2\n",
    "num_cols = X2.columns[X2.dtypes != \"object\"]\n",
    "cat_cols = X2.columns[X2.dtypes == \"object\"]\n",
    "\n",
    "# train-test-split\n",
    "X_train2, X_test2, y_train, y_test = pp.split(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV object\n",
    "grid_search_xgb2 = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=cv,\n",
    "    scoring=ftwo_scorer,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    "    error_score='raise'\n",
    ")\n",
    "grid_search_xgb2.fit(X_train2, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params_xgb2 = grid_search_xgb2.best_params_\n",
    "best_est_xgb2 = grid_search_xgb2.best_estimator_\n",
    "print(\"Best params for XGB are:\", best_params_xgb2)\n",
    "print(\"Best est for XGB are:\", best_est_xgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_opt =(\n",
    "    xgb_pipeline,\n",
    "    param_grid=best_params_xgb,\n",
    "    cv=cv,\n",
    "    scoring=ftwo_scorer,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    "    error_score='raise'\n",
    ")\n",
    "grid_search_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: \n",
    "Best params for XGB are: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 100}\n",
    "Best est for XGB are: Pipeline(steps=[('preprocessor',\n",
    "                 ColumnTransformer(remainder='passthrough',\n",
    "                                   transformers=[('scaler', MinMaxScaler(),\n",
    "                                                  Index(['sp_fix_count', 'sp_fix_duration_ms_total', 'sp_fix_duration_ms_mean',\n",
    "       'sp_fix_duration_ms_var', 'sp_len_px_total',\n",
    "       'sp_saccade_amplitude_px_mean', 'sp_saccade_amplitude_px_var',\n",
    "       'sp_distance_to_centre_px_mean', 'sp_distance_to_centre_px_...\n",
    "                               feature_types=None, gamma=None, grow_policy=None,\n",
    "                               importance_type=None,\n",
    "                               interaction_constraints=None, learning_rate=0.1,\n",
    "                               max_bin=None, max_cat_threshold=None,\n",
    "                               max_cat_to_onehot=None, max_delta_step=None,\n",
    "                               max_depth=5, max_leaves=None,\n",
    "                               min_child_weight=None, missing=nan,\n",
    "                               monotone_constraints=None, multi_strategy=None,\n",
    "                               n_estimators=100, n_jobs=None,\n",
    "                               num_parallel_tree=None, random_state=None, ...))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data into DMatrix format (optimized data structure for XGBoost)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Setting parameters for XGBoost model\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # for binary classification\n",
    "    'eval_metric': 'logloss',         # evaluation metric\n",
    "    'max_depth': 3                     # maximum depth of trees\n",
    "}\n",
    "\n",
    "\n",
    "Next, we convert the data into the DMatrix format, which is the optimized data structure for XGBoost.\n",
    "We define parameters for the XGBoost model, such as the objective function, evaluation metric, and maximum depth of trees.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
